1，cloud 平台
在 Cloud 平台的工作主要是负责快速搭建部署大模型并提供推理服务，然后集成到平台上面。

当时原来的部署方式是，每次要部署一个新的大模型，都是新建一个子模块来编写模型的加载和推理方式，
然后结合web应用框架把推理服务串联起来以这样一种方式来进行搭建

考虑到未来会有更多的大模型加入进来以及减少重复工作的繁琐，于是找时间调研了网上有没有类似的一个东西，
后面就找到了 FastChat 这个开源框架非常适合公司目前的情况和需求，它里面集合了目前最先进模型的训练和评估代码和具有 Web UI 和 OpenAI 兼容 RESTful API 的分布式多模型服务系统

当然我们主要用的还是它里面的 worker 核心功能，它把目前流行的大模型原生加载方式和conversation的template都做好了封装，
同时也集成了RESTful API web服务，于是我们就把FastChat作为底层框架，梳理我们目前在用的模型也填充进去，
包括一些推理逻辑流程处理，就这样把模型统一部署做起来了


2，


3，当我们进行 AI 推理的时候，往往需要经历这么两个步骤
    当我们有了这个训练模型之后，我们首先需要对这个模型本身进行一些优化，我们不太可能把这个原生的训练模型直接用原生的框架部署上去，
    因为那样的话效率太低了，我们往往需要对这个模型去进行一些优化，之后呢有了优化的模型，可能会有多种多样不同的模型，
    基于不同框架的模型，然后我们放在一个模型的池子 model repo 里
    那么下面一个需要解决的问题就是说，我们如何有效的把这些模型 serving 起来，如何把这些模型部署起来，然后去对外提供服务
    并且提供服务的时候我还能够保证一个高效以及高质

    在模型优化这块，我们提供了 tensorRT 这么一个工具，它可以帮助把我们训练好的模型优化成一个更加高效更加高性能的推理的引擎，然后来进行推理
    tensorRT 可以接受来自于不同框架的模型，包括 pytorch、TensorFlow、PaddlePaddle、mxnet、onnx 等等
    那么你的模型是这些训练框架模型的格式保存下来的话都可以通过 tensorRT 进行优化
    同时 tensorRT 可以在保证精度的情况下对整个模型的吞吐量、准确性以及 memory 的使用进行一个优化，使得这个模型能够达到一个高效
    并且 tensorRT 可以针对不同的 GPU 硬件去进行优化的，不同的 GPU 架构以及边缘端等等

    那么当我们使用 tensorRT 来进行优化的时候，它是可以对多种不同的模型都达到一个比较好的优化效果
    无论是 Computer Vision、Speech Recognition、NLP、强化学习、Text-to-Speech 以及推荐的模型都能达到一个几十倍甚至上百倍的相较于 CPU 加速效果

    当我们有了这个优化好的模型之后，下一步就是说把这些模型部署起来然后对外提供服务，并且能够充分地去使用我们的硬件资源
    一方面是提供更多的吞吐、另外一方面降低整体的成本，那么这个时候我们可以通过 triton 来实现

    triton 是一个开源框架，是 NVIDIA 推出的，主要针对的就是更加高效、更加简洁的进行推理 serving 服务
    triton 可以 serving 多种不同框架的模型的，比如说 TensorFlow、pytorch、tensorRT、XGBoost、onnx 等等
    这些模型都是可以通过 triton 来部署的
    
    之后是它可以接收各种各样的请求类型，比方说 real time 的请求、批处理的请求、流处理的请求，都是可以的
    之后就是 triton 支持多种多样的平台的，无论是 CPU、GPU，操作系统也是 Linux、Windows 甚至是 virtualization(虚拟机) 都是可以的
    以及云端、边缘端（主要指 jetson 设备），都是可以通过 triton 来进行模型部署的

    之后就是 triton 本身可以是很好的跟一些 DevOps 以及 MLOps 的这些工具和生态进行结合的
    它可以很好的跟 k8s、KServe、普罗米修斯 Prometheus 以及 Grafana 来进行结合从而实现一个大规模的部署

    最后就是也是最重要的一点就是使用 triton 我们可以去提升整体的一个 serving 的性能，并且提高 GPU 的利用率
    那么在 triton 当中呢我们通过 Model Analyzer（模型分析仪）、optimal model configuration（最优模型配置最优模型配置）等策略来提升在线推理的性能，
    可以提供更高的吞吐量同时呢充分的去利用 GPU 以及 CPU 的资源
