1，cloud 平台
在 Cloud 平台的工作主要是负责搭建部署大模型并提供推理服务，然后集成到平台上面。

当时原来的部署方式是，每来一个新的大模型，都要编写这个大模型的加载和推理方式
然后集成web框架把推理服务串联起来以这样一种方式来进行部署，来对外提供服务

在当时考虑到未来会有更多海量大模型部署以及减少重复工作的成本，于是开始调研市面上的这些推理框架
后面就找到了 FastChat 这个开源框架，它是去年非常知名的一个大语言模型框架，是一个用于训练、部署和评估基于大型语言模型的聊天机器人的开放平台
不仅提供了大语言模型全量参数微调、Lora参数微调、模型推断、模型量化、模型部署及调度等全套的源代码

结合公司内部当前正在应用和准备实验的大语言模型，比较符合公司的需求，所以我们就按照 FastChat 设计了一套我们自己的推理框架
通过 controller、model_worker 以及适配器组件构建了统一部署框架，来推动模型部署推理服务这一块的快速和节省时间成本
这样把模型统一部署做起来了

FastChat Serving with Web GUI 启动方式
    1，fastchat/serve/controller.py
        1.1，model_adapters 数组
            注册模型适配器类
            适配器类包含有模型匹配方法、模型加载、匹配对话模板方法等

        1.2，conv_templates 数组
            注册对话模板类
            模板类主要包括对话双方提示的信息

    2，fastchat/serve/model_worker.py
        2.1，根据启动命令的路径参数来加载到 controller 容器里的对应模型
        2.2，针对模型的状态 健康检查心跳
        2.3，封装的 API，包括获取模型的状态、详情、根据请求 input 生成响应或者流响应

    3，fastchat/serve/gradio_web_server.py || fastchat/serve/gradio_web_server_multi.py
        3.1，model_info 字典
            注册模型信息，包括模型名、描述、官方链接等
        3.2，根据启动命令的 controller-url 拿到当前已经注册的所有模型
        3.3，根据浏览器上选择的模型发起对话和响应


2，引擎平台
    引擎平台主要是要把这些训练好的大模型通过 k8s 来实现自动部署
    训练好的大模型会存储在统一的模型池子里，当新创建一个大模型推理服务时，
    目标
        支持nlp/cv 方向 小模型/大模型
        支持单模型一键部署，提供Http/RPC服务
        支持离线SDK
        支持离线镜像
        支持扩缩容
        支持自定义负载均衡规则
        兼容多模型格式
        兼容多模型Graph部署


3，当我们进行 AI 推理的时候，往往需要经历这么两个步骤
    当我们有了这个训练模型之后，我们首先需要对这个模型本身进行一些优化，我们不太可能把这个原生的训练模型直接用原生的框架部署上去，
    因为那样的话效率太低了，我们往往需要对这个模型去进行一些优化，之后呢有了优化的模型，可能会有多种多样不同的模型，
    基于不同框架的模型，然后我们放在一个模型的池子 model repo 里
    那么下面一个需要解决的问题就是说，我们如何有效的把这些模型 serving 起来，如何把这些模型部署起来，然后去对外提供服务
    并且提供服务的时候我还能够保证一个高效以及高质

    在模型优化这块，我们提供了 tensorRT 这么一个工具，它可以帮助把我们训练好的模型优化成一个更加高效更加高性能的推理的引擎，然后来进行推理
    tensorRT 可以接受来自于不同框架的模型，包括 pytorch、TensorFlow、PaddlePaddle、mxnet、onnx 等等
    那么你的模型是这些训练框架模型的格式保存下来的话都可以通过 tensorRT 进行优化
    同时 tensorRT 可以在保证精度的情况下对整个模型的吞吐量、准确性以及 memory 的使用进行一个优化，使得这个模型能够达到一个高效
    并且 tensorRT 可以针对不同的 GPU 硬件去进行优化的，不同的 GPU 架构以及边缘端等等

    那么当我们使用 tensorRT 来进行优化的时候，它是可以对多种不同的模型都达到一个比较好的优化效果
    无论是 Computer Vision、Speech Recognition、NLP、强化学习、Text-to-Speech 以及推荐的模型都能达到一个几十倍甚至上百倍的相较于 CPU 加速效果

    当我们有了这个优化好的模型之后，下一步就是说把这些模型部署起来然后对外提供服务，并且能够充分地去使用我们的硬件资源
    一方面是提供更多的吞吐、另外一方面降低整体的成本，那么这个时候我们可以通过 triton 来实现

    triton 是一个开源框架，是 NVIDIA 推出的，主要针对的就是更加高效、更加简洁的进行推理 serving 服务
    triton 可以 serving 多种不同框架的模型的，比如说 TensorFlow、pytorch、tensorRT、XGBoost、onnx 等等
    这些模型都是可以通过 triton 来部署的
    
    之后是它可以接收各种各样的请求类型，比方说 real time 的请求、批处理的请求、流处理的请求，都是可以的
    之后就是 triton 支持多种多样的平台的，无论是 CPU、GPU，操作系统也是 Linux、Windows 甚至是 virtualization(虚拟机) 都是可以的
    以及云端、边缘端（主要指 jetson 设备），都是可以通过 triton 来进行模型部署的

    之后就是 triton 本身可以是很好的跟一些 DevOps 以及 MLOps 的这些工具和生态进行结合的
    它可以很好的跟 k8s、KServe、普罗米修斯 Prometheus 以及 Grafana 来进行结合从而实现一个大规模的部署

    最后就是也是最重要的一点就是使用 triton 我们可以去提升整体的一个 serving 的性能，并且提高 GPU 的利用率
    那么在 triton 当中呢我们通过 Model Analyzer（模型分析仪）、optimal model configuration（最优模型配置最优模型配置）等策略来提升在线推理的性能，
    可以提供更高的吞吐量同时呢充分的去利用 GPU 以及 CPU 的资源
