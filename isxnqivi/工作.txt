在 Cloud 平台的工作主要是负责快速搭建部署大模型并提供推理服务，然后集成到平台上面。

当时原来的部署方式是，每次要部署一个新的大模型，都是新建一个子模块来编写模型的加载和推理方式，
然后结合web应用框架把推理服务串联起来以这样一种方式来进行搭建

考虑到未来会有更多的大模型加入进来以及减少重复工作的繁琐，于是找时间调研了网上有没有类似的一个东西，
后面就找到了 FastChat 这个开源框架非常适合公司目前的情况和需求，它里面集合了目前最先进模型的训练和评估代码和具有 Web UI 和 OpenAI 兼容 RESTful API 的分布式多模型服务系统

当然我们主要用的还是它里面的 worker 核心功能，它把目前流行的大模型原生加载方式和conversation的template都做好了封装，
同时也集成了RESTful API web服务，于是我们就把FastChat作为底层框架，梳理我们目前在用的模型也填充进去，
包括一些推理逻辑流程处理，就这样把模型统一部署做起来了