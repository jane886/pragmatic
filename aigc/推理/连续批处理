Continuous Batching

连续批处理：连续批处理是一种优化技术，它允许在生成过程中动态地调整批处理的大小。
具体来说，一旦一个序列在批处理中完成生成，就可以立即用新的序列替代它，从而提高了 GPU 的利用率。
这种方法的关键在于实时地适应当前的生成状态，而不是等待整个批次的序列都完成。


出于效率考虑，推理框架一般会将第1次推理(生成第1个Token)和余下的推理（生成其余Token）分别设计为Prefill和Decode2个过程。

Prefill是将1个请求的Prompt一次性转换为KV Cache,并生成第1个Token的过程。
中间过程计算得到的K、V将被保留在显存中（即KV Cache，用于避免后续Decode过程重复计算这些K、V导致算力浪费）
从第2个Token开始，将上一次推理的输出（新生成的1个Token）作为输入进行一次新的推理，这就是Decode的过程。
很明显，将推理分为Prefill和Decode2个流程，是考虑到生成第1个Token和其余Token时计算模式的差异较大，分开实现有利于针对性的优化。

TGI是较早引入Continuous Batching特性的框架，其中请求的合并和剔除就是通过Router向Server发送Concatenate和Filter的Request实现的。


由于 LLM 巨大的 GPU 内存开销和计算成本，在大多数应用中，机器学习工程师通常通过内部调整（如量化和对 CUDA 核的定制）来优化。
然而，由于 LLM 通过迭代生成其输出，并且 LLM 推理通常涉及内存而不是计算，
因此在很多实践中，优化系统级批处理可以使性能差异达到10倍甚至更多。

一种最近提出的优化方法是连续批处理（Continuous batching），也称为动态批处理或基于迭代级的批处理。
其具有如下惊人的效果：
    基于vLLM，使用连续批处理和连续批处理特定的内存优化，可以实现多达23倍的吞吐量提升；

    对于 HuggingFace 本地生成推理，使用连续批处理，可以实现8倍的吞吐量提升；

    基于 NVIDIA 的 FasterTransformer，使用优化过的模型实现，可以实现4倍的吞吐量提升。


LLM 推理是内存 IO 限制，而不是计算限制。换句话说，目前加载 1MB 的数据到 GPU 所需的时间比 1MB 的数据在GPU上计算所需的时间长。
这意味着 LLM 推理的吞吐量很大程度上取决于您能将多少批数据装入到高速GPU 内存中；

GPU 内存的消耗量随着基本模型大小和标记长度的增加而增加。如果我们将序列长度限制为 512，那么在一个批处理中，
我们最多只能处理28个序列；一个序列长度为 2048 则批处理大小最多只能为7个序列。
需要注意的是，这只是一个上限，因为中间计算结果没有留下存储的空间。

这意味着优化内存使用有很多余地。这就是为什么像 AutoGPTQ 这样的模型量化策略具有如此强大的力量的原因；
如果您可以将内存使用量减半，您将能够为更大的批处理提供更多的空间。
然而，并不是所有的策略都需要对模型权重进行修改。
例如，FlashAttention 通过重新组织注意计算来减少内存IO，从而发现显著的性能提升。


连续批处理是另一种内存优化技术，它不需要对模型权重进行修改。
接下来，我们解释一下连续批处理如何工作，以及如何提高LLM生成过程的内存效率。

推断过程：
    当我们谈论大型语言模型（LLM）的推断过程时，我们指的是使用已经训练好的模型来对输入文本进行处理，
    从而生成相应的输出。推断过程涉及将一个或多个文本片段传递给模型，并从模型中获取相应的预测或生成的文本。

    在传统的批处理策略中，文本通常会被分成小批次（batch）进行处理，以便在 GPU 或其他硬件上进行并行计算。
    然而，由于 LLMs 通常需要大量的内存和计算资源，传统的批处理策略可能会导致一些低效性：

        内存消耗高：传统批处理策略可能会导致大量的 GPU 内存被占用，限制了可以同时处理的文本量。

        计算资源未被充分利用：由于内存限制，传统批处理策略可能导致 GPU 计算资源未被充分利用，从而降低了推断效率。

        高延迟：由于大型模型的计算需求，传统批处理策略可能会导致高延迟，使得生成结果的响应速度变慢。

        难以处理长文本：传统批处理策略可能会限制模型处理长文本的能力，因为它们可能无法一次性将整个文本载入内存。
    因此，传统的批处理策略可能会限制了 LLMs 在实际应用中的效率和性能。

    LLMs 通常是内存受限而不是计算受限时，他指的是在 LLM 推断过程中，通常更多地受到可用内存的限制，而不是计算能力的限制。

    内存受限意味着在处理大型语言模型时，系统的内存资源是一个相对稀缺的资源。
    这意味着模型在推断时需要将许多数据存储在内存中，例如输入文本、中间计算结果等。
    如果内存不足以容纳所需的数据，可能会导致内存溢出或性能下降。

    相比之下，计算受限指的是在进行模型推断时，计算资源（例如 CPU 或 GPU 的处理能力）是主要的瓶颈。
    这种情况下，系统的处理能力会成为推断性能的主要限制因素，而内存资源可能并不是主要的瓶颈。

    因此，在 LLM 推断中，通常更关键的是如何有效地利用有限的内存资源，而不是解决计算资源瓶颈。
    通过优化内存的使用方式，可以使得在实际工作负载中推断性能提升 10 倍甚至更多。
    这意味着通过合理地调度和利用内存，可以显著地提高 LLM 模型在实际应用中的性能表现。


连续批处理是一种最近提出的优化方法，也称为动态批处理或迭代级别调度批处理。
它旨在解决传统批处理策略中的一些低效性问题。

传统批处理策略通常是基于请求的动态批处理，即一次性处理一批请求。
这可能会导致一些请求在推断过程中花费的时间较长，因为某些请求可能会比其他请求更加复杂或耗时。

相比之下，连续批处理采用了一种更为灵活的方法。它允许在推断过程中动态地调整批次的大小，以适应不同请求的复杂程度。具体来说，连续批处理会在模型推断的过程中不断地将新的请求添加到当前的批次中，同时保持一定的效率。

这意味着，如果某些请求需要更多时间来完成推断，它们可以在当前批次中等待，而不会等待整个批次处理完毕。
这样可以显著降低高复杂度请求的等待时间，提高了推断的效率。

