与传统的 CNN 模型推理不同，大语言模型的推理通常会分成 prefill 和 decoding 两个阶段。

每一个请求发起后产生的推理过程都会先经历一个 Prefill 过程，prefill 过程会计算用户所有的输入，并生成对应的 KV 缓存，
再经历若干个 decoding 过程，每一个 decoding 过程，服务器都会生成一个字符，并将其放入到 KV 缓存当中，之后依次迭代。

由于 decoding 过程是逐个字符生成的，每一段答案的生成都需要很长时间，会生成很多字符，所以 decoding 阶段的数量非常多，
占到整个推理过程的 90% 以上。

在 Prefill 过程中，虽然计算量很大，因为要一次性完成用户输入的所有词的计算，但它只是一次性的过程，所以在整个推理中只占不到 10% 的时间。

在大语言模型推理中常会用到四个指标：Throughput（吞吐量）、First Token Latency（首字延迟）、Latency（延迟）和QPS（每秒请求数）。
这四个性能指标会从四个不同的方面来衡量一个系统的服务提供能力。



先来详细介绍一下大语言模型的推理过程，前文中提到了每个请求都要经历 prefill 和 decoding 两个阶段
在 prefill 阶段，至少要做四件事情：

    第一件事情是把用户的输入进行向量化，tokenize 的过程指的是将用户输入的文本转换为向量，
    相对于 prefill 整个阶段来说，大概要占掉 10% 的时间，这是有代价的。

    之后就会进行真正的 prefill 计算，这一过程会占掉大概 80% 的时间。

    计算之后会进行 sampling，这个过程在 Pytorch 里面一般会用 sample、top p。
    在大语言模型推理当中会用 argmax。总而言之，是根据模型的结果，生成最后词的一个过程。这个过程会占掉 10% 的时间。

    最后将 refill 的结果返回给客户，这需要的时间会比较短，大概占 2% 到 5% 的时间。

    Decoding 阶段不需要 tokenize，每一次做 decoding 都会直接从计算开始，整个decoding 过程会占掉 80% 的时间，
    而后面的 sampling，也就是采样生成词的过程，也要占掉 10% 的时间。

    但它会有一个 detokenize 的时间，detokenize 是指生成了一个词之后，这个生成的词是个向量，
    需要把它解码回文本，这一操作大概会占掉 5% 的时间，最后将这个生成的词返回给用户。

    新的请求进来，在进行完 prefill 之后，会不断迭代进行 decoding，每一个 decoding 阶段结束之后，都会将结果当场返回给客户。
    这样的生成过程在大语言模型里面是很常见的，我们称这样的方式为流式传输。