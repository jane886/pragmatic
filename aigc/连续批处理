LLMs 在现实应用中的计算成本主要由服务成本所主导，但是传统的批处理策略存在低效性。
Continuous Batching 连续批处理成为了解决这一问题的新方法，这个技术利用内存，而不是计算能力，
来实现 10 倍以上的性能提升，将改变AI领域的游戏规则。


当我们谈论大型语言模型（LLM）的推断过程时，我们指的是使用已经训练好的模型来对输入文本进行推理，从而生成相应的输出。
对于每个请求：
    从一组token（称为“前缀”或“提示”）开始。
    LLM产生一组完成token，只有在产生停止标记或达到最大序列长度后才停止。
这是一个迭代过程。对于模型的每次新的前向传递，您可以获得一个额外的完成token。


Q1. 你能解释一下什么是大型语言模型（LLM）的推断过程吗？以及在传统批处理策略中存在的哪些低效性？
    当我们谈论大型语言模型（LLM）的推断过程时，我们指的是使用已经训练好的模型来对输入文本进行处理，从而生成相应的输出。
    推断过程涉及将一个或多个文本片段传递给模型，并从模型中获取相应的预测或生成的文本。

    在传统的批处理策略中，文本通常会被分成小批次（batch）进行处理，以便在 GPU 或其他硬件上进行并行计算。
    然而，由于 LLMs 通常需要大量的内存和计算资源，传统的批处理策略可能会导致一些低效性：
        内存消耗高：传统批处理策略可能会导致大量的 GPU 内存被占用，限制了可以同时处理的文本量。
        计算资源未被充分利用：由于内存限制，传统批处理策略可能导致 GPU 计算资源未被充分利用，从而降低了推断效率。
        高延迟：由于大型模型的计算需求，传统批处理策略可能会导致高延迟，使得生成结果的响应速度变慢。
        难以处理长文本：传统批处理策略可能会限制模型处理长文本的能力，因为它们可能无法一次性将整个文本载入内存。

    因此，传统的批处理策略可能会限制了 LLMs 在实际应用中的效率和性能。连续批处理（continuous batching）作为一种优化策略，以解决传统批处理策略中存在的这些低效性问题。


Q2. 为什么传统的处理方法可能会导致在实际应用中服务成为大多数情况下的计算成本主导因素？
    传统的处理方法将 LLMs 视为“黑匣子”，主要通过内部更改（如量化和自定义 CUDA 内核）来进行优化。
    这种方法忽视了 LLMs 在推断过程中生成输出的迭代性质，以及 LLM 推断通常受限于内存而不是计算资源。
    由于 LLMs 通常需要大量的 GPU 内存和计算成本，这导致在实际应用中，服务成为计算成本的主导因素。

    因为 LLMs 在推断过程中需要迭代生成输出，而且通常情况下是内存受限的，所以存在着可以在系统级别进行批处理优化的机会。
    这意味着可以通过合理的批处理策略来最大程度地利用 GPU 资源，从而显著提高推断吞吐量，降低计算成本，使得服务成本不再是主导因素。
