
4. Freeze 监督微调方法

    4.1 Freeze 微调方法的概念
        Freeze 方法，即参数冻结，对原始模型部分参数进行冻结操作，仅训练部分参数，以达到在单卡或不进行 TP 或 PP 操作，就可以对大模型进行训练。
        在语言模型模型微调中，Freeze 微调方法仅微调 Transformer 后几层的全连接层参数，而冻结其它所有参数。

    4.2 Freeze 微调方法的原理
        Freeze 微调方法为什么只微调 Transformer 后几层的全连接层参数呢？下面对其原因进行展开讲述。

        Transformer 模型主要由自注意力层和全连接层（FF 层）构成。
        对于 Transformer 的每一层结构，自注意力层的参数量为4⋅d2，即WQ、WQ、WQ和WQ ∈Rd×d；
        FF 层的参数量为8⋅d2，即W1​∈Rd×4d,W2​∈Rd×4d。因此 FF 层占据了模型的32​的参数，具有重要的研究价值。

    4.3 Freeze 微调方法的优势
        大量减少了大语言模型的微调参数，是一种参数高效的微调方法；
        由于只需微调高层特征，加快了模型的收敛，节约了微调的时间；
        最大程度地保留了大语言模型预训练所学习到的语言的 “共性”，可解释性较强。