
3. P-tuning v2 微调方法

    3.1 P-tuning v2 微调方法的相关技术
    传统的微调方法需要微调整个预训练语言模型，对于大语言模型的微调需要大量的资源和时间，急需更加高效的微调方法。
    理解 P-tuning v2 微调方法，首先需要了解 prefix-tuning 微调方法和 P-tuning v1 微调方法。
        
        3.1.1 Prefix-tuning 微调方法
            Prefix-tuning 微调方法在模型中加入 prefix，即连续的特定任务向量，微调时只优化这一小段参数。
        3.1.2 P-tuning v1 微调方法
            P-tuning v1 微调方法是将 Prompt 加入到微调过程中，只对 Prompt 部分的参数进行训练，而语言模型的参数固定不变。
        3.1.3 存在不足
            P-tuning v1 微调方法缺少普遍性。实验表明，当模型规模超过 100 亿个参数时，P-tuning v1 可以与全参数微调方法相媲美，
            但对于那些较小的模型，P-tuning v1 方法和全参数微调方法的表现有很大差异，效果很差。
            同时，P-tuning v1 缺少跨任务的通用性，在序列标注任务中的有效性没有得到验证。
            序列标注需要预测一连串的标签，而且大都是无实际意义的标签，对于 P-tuning v1 微调方法极具挑战。
            此外，当模型层数很深时，微调时模型的稳定性难以保证。模型层数越深，第一层输入的 prompt 对后面的影响难以预估。

    3.2 P-tuning v2 微调方法的原理
        P-tuning v2 微调方法是 P-tuning v1 微调方法的改进版，同时借鉴了 prefix-tuning 微调的方法。
        与 P-tuning v1 微调方法相比，P-tuning v2 微调方法采用了 prefix-tuning 的做法，在输入前面的每一层都加入可微调的参数。
        在 prefix 部分，每一层的 transformer 的 embedding 输入都需要被微调，而 P-tuning v1 只在第一层进行微调。
        同时，对于 prefix 部分，每一层 transformer 的输入不是从上一层输出，而是随机初始化的 embedding 作为输入。

        此外，P-Tuning v2 还包括以下改进：

            移除 Reparamerization 加速训练方式；
            采用多任务学习优化：基于多任务数据集的 Prompt 进行预训练，然后再适配的下游任务。
            舍弃词汇 Mapping 的 Verbalizer 的使用，重新利用 [CLS] 和字符标签，跟传统微调方法一样利用 cls 或者 token 的输出做自然语言理解，
            以增强通用性，可以适配到序列标注任务。
    
    3.3 P-tuning v2 微调方法优点
        P-tuning v2 微调方法解决了 P-tuning v1 方法的缺陷，是一种参数高效的大语言模型微调方法。

        P-tuning v2 微调方法仅精调 0.1% 参数量（固定 LM 参数），在各个参数规模语言模型上，均取得和 Fine-tuning 相比肩的性能，
        解决了 P-tuning v1 在参数量不够多的模型中微调效果很差的问题。