2. LoRA 微调方法
    2.1 LoRA 微调方法的基本概念
        LoRA（Low-Rank Adaptation of Large Language Models），直译为大语言模型的低阶自适应。
        LoRA 的基本原理是冻结预训练好的模型权重参数，在冻结原模型参数的情况下，通过往模型中加入额外的网络层，并只训练这些新增的网络层参数。
        由于这些新增参数数量较少，这样不仅 finetune 的成本显著下降，还能获得和全模型参数参与微调类似的效果。

        随着大语言模型的发展，模型的参数量越来越大，比如 GPT-3 参数量已经高达 1750 亿，因此，微调所有模型参数变得不可行。
        LoRA 微调方法由微软提出，通过只微调新增参数的方式，大大减少了下游任务的可训练参数数量。

    2.2 LoRA 微调方法的基本原理
        神经网络的每一层都包含矩阵的乘法。这些层中的权重矩阵通常具有满秩（zhi第四声）。
        当适应特定任务时，预训练语言模型具有低的 “内在维度”，将它们随机投影到更小的子空间时，它们仍然可以有效地学习。
        在大语言模型微调的过程中，LoRA 冻结了预先训练好的模型权重，并将可训练的秩的分解矩阵注入到 Transformer 体系结构的每一层。

    2.3 LoRA 微调方法的主要优势
        预训练模型参数可以被共享，用于为不同的任务构建许多小的 LoRA 模块。
        冻结共享模型，并通过替换矩阵 A 和 B 可以有效地切换任务，从而显著降低存储需求和多个任务切换的成本。
        当使用自适应优化器时，由于不需要计算梯度以及保存太多模型参数，LoRA 使得微调效果更好，并将微调的硬件门槛降低了 3 倍。
        低秩分解采用线性设计的方式使得在部署时能够将可训练的参数矩阵与冻结的参数矩阵合并，与完全微调的方法相比，不引入推理延迟。
        LoRA 与其它多种微调方法不冲突，可以与其它微调方法相结合，比如下节实训将要介绍的前缀调优方法等。