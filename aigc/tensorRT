当我们有了这个训练模型之后，我们首先需要对这个模型本身进行一些优化，我们不太可能把这个原生的训练模型直接用原生的框架部署上去，
因为那样的话效率太低了，我们往往需要对这个模型去进行一些优化，之后呢有了优化的模型，可能会有多种多样不同的模型，
基于不同框架的模型，然后我们放在一个模型的池子 model repo 里
那么下面一个需要解决的问题就是说，我们如何有效的把这些模型 serving 起来，如何把这些模型部署起来，然后去对外提供服务
并且提供服务的时候我还能够保证一个高效以及高质

在模型优化这块，我们提供了 tensorRT 这么一个工具，它可以帮助把我们训练好的模型优化成一个更加高效更加高性能的推理的引擎，然后来进行推理
tensorRT 可以接受来自于不同框架的模型，包括 pytorch、TensorFlow、PaddlePaddle、mxnet、onnx 等等
那么你的模型是这些训练框架模型的格式保存下来的话都可以通过 tensorRT 进行优化
同时 tensorRT 可以在保证精度的情况下对整个模型的吞吐量、准确性以及 memory 的使用进行一个优化，使得这个模型能够达到一个高效
并且 tensorRT 可以针对不同的 GPU 硬件去进行优化的，不同的 GPU 架构以及边缘端等等

那么当我们使用 tensorRT 来进行优化的时候，它是可以对多种不同的模型都达到一个比较好的优化效果
无论是 Computer Vision、Speech Recognition、NLP、强化学习、Text-to-Speech 以及推荐的模型都能达到一个几十倍甚至上百倍的相较于 CPU 加速效果
