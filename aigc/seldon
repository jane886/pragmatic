优点：
    原生基于k8s，支持多模型框架，支持灰度/金丝雀等发布方案
    seldon core基于k8s，无需任何改造；
    支持DAG推理图；
    代码的侵入性非常低。用户只需要提供对应的 HTTP/gRPC Endpoint 来提供服务就可以，而不需要引用 Seldon Core 的 SDK；
缺点：
    mlflow推理框架基于seldon core提供的基础镜像，没有任何针对大模型优化；
    无需推理图情况，也会有engine网络损耗；
    不直接支持离线部署；
目标
    支持nlp/cv 方向 小模型/大模型
    支持单模型一键部署，提供Http/RPC服务
    支持离线SDK
    支持离线镜像
    支持扩缩容
    支持自定义负载均衡规则
    兼容多模型格式
    兼容多模型Graph部署
未来
seldon-core-microservice本身对于模型load和infer/predict的工作，支持完全交给用户自定义。
是否可以在FastChat/其他推理框架上优化推理框架本身，结合seldon core推理服务。


根据提供的资料,使用Seldon部署大模型的主要优缺点如下:
    优点:
    统一的部署方式:
        Seldon Core提供了一套相对统一的部署方式,支持不同机器学习框架(TensorFlow、PyTorch等)训练的模型。
        这简化了不同框架下模型的部署过程。
    支持复杂的推理图:
        Seldon Core支持构建有向无环的推理图,可以实现模型的解释、异常检测、特征转换等复杂的推理流程。
        这对于部署大型模型pipeline很有帮助。
    与Kubernetes集成良好:
        Seldon Core基于Kubernetes,可以很好地与Kubernetes集成,利用Kubernetes的扩展性和高可用性。
        这使得Seldon Core能够支持大规模模型部署和管理。
    
    缺点:
    依赖Kuberne tes:
        Seldon Core强依赖Kubernetes,如果您不使用Kubernetes,Seldon Core可能不是最佳选择。
    复杂性:
        尽管Seldon Core提供了统一的部署方式,但对于新手来说,学习和配置Seldon Core可能会有一定难度。
    性能:
        对于一些对延迟要求很高的实时应用,Seldon Core的性能可能不如一些专门针对模型推理优化的工具,
        如TensorFlow Serving。
总的来说,Seldon Core在部署大型模型方面提供了很好的支持,特别是对于需要复杂推理流程的场景。
但它也存在一定的局限性,需要根据具体需求进行权衡。


Seldon Core：速度极快、行业就绪的机器学习

1，一个开源平台，用于在 Kubernetes 上大规模部署机器学习模型。
2，Seldon 核心将您的 ML 模型（Tensorflow、Pytorch、H2o 等）或语言包装器（Python、Java 等）转换为生产 REST/GRPC 微服务。
3，Seldon 可以扩展到数千个生产机器学习模型，并提供开箱即用的高级机器学习功能，包括高级指标、请求日志记录、解释器、离群值检测器、A/B 测试、金丝雀等。
4，通过我们预先打包的推理服务器和语言包装器，可以简化使用 Seldon Core 部署模型的过程。
    MLFLOW_SERVER
    SKLEARN_SERVER
    TEMPO_SERVER
    TENSORFLOW_SERVER
    TRITON_SERVER
    XGBOOST_SERVER
    自定义推理服务器--CUSTOM_INFERENCE_SERVER

5，Seldon Core 的主要组件：
    可重用和不可重用模型服务器
    用于容器化模型的语言包装器
    SeldonDeployment CRD 和 Seldon Core Operator
    用于高级推理图的服务编排器
    以及与第三方系统的集成：
        Kubernetes Ingress 与Ambassador 和 Istio 集成
        Metrics with Prometheus Prometheus 的指标
        Tracing with Jaeger  与 Jaeger 一起追踪
        OpenApi 端点文档

6，为什么我不直接用 Flask 包装我的模型呢？
    以下是选择 Seldon Core 的一些好处：
        所有艰苦的工作已经完成
        开箱即用的复杂推理图
        可重用的模型服务器（构建一次，部署多次）
        与指标和跟踪解决方案集成
        自动入口配置
        Seldon Core 经过了由开源和商业用户组成的广泛社区的考验

7，SeldonDeployment（简称 sdep）的 pod 中都包含一个 engine 容器。
这个 engine 容器，是由 seldon-manager 服务为每个 sdep 插入的，
用于满足复杂推理场景的需求，如合并(Combine)多个模型推理结果，推理请求处理（Transform-Input），推理结果转换（Transform-Output），以及推理请求路由等对模型推理的请求和结果进行编排

8，启动过程很清晰简洁的，主要包含如下 4 个步骤：
    定义及解析命令行参数
    加载模型
    定义 Rest 服务
    启动进程
  当 sdep 资源创建后，manager 服务为其创建了一个 pod。这个 pod 包含了四个 container
  四个 container 的作用和关系，分别如下：
    init 容器：根据 sdep 的参数 modelUri，从 gs 获取模型文件，并保存到/mnt/models

    tfserving 容器：从/mnt/models 加载模型，并通过 2000/2001 端口提供服务

    mnist-model 容器：从其使用的 image 可知，这是一个 proxy 服务，主要实现协议转换，即 seldon 协议请求，转换成 tensorflow 协议请求

    engine 容器：主要用于支持 sdep 资源的 graph 功能，即可以将多个模型服务分别进行打分及组合。

9，seldon 的 manager 服务，即 seldon-controller-manager，这是一个 k8s 的 Operator 服务，
    负责管理所有 SeldonDeployment（简称 sdep）资源的生命周期，包括创建，修改及删除。
    每当新的 sdep 资源被创建时，manager 服务会为其创建相关的 deployment 及 service。





 